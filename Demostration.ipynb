{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import precision_recall_fscore_support, mean_squared_error, r2_score\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool, CatBoostRegressor\n",
    "import plotly\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимые данные для выборок. Список необходимых фич для команд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cols_order.txt') as f:\n",
    "    correct_order = f.read().splitlines() \n",
    "\n",
    "with open('for_ht.txt') as f:\n",
    "    columns_for_ht = f.read().splitlines() \n",
    "    \n",
    "with open('for_at.txt') as f:\n",
    "    columns_for_at = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'italy_matches.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные собирались и агрегировались мною (python + selenium), здесь будет только результат. Но если необходимо, можно опубликовать и парсер. \n",
    "\n",
    "Данные по матчам собраны за последние 8 сезонов в рамках одной футбольной лиги (Серия А)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(FILE_NAME, index_col=[0], parse_dates=['match_date'])\n",
    "data = data.reset_index()\n",
    "data = data.drop('index', axis=1)\n",
    "\n",
    "data = data.drop_duplicates(subset='match_id')\n",
    "\n",
    "# Вычислим победителя матча. \n",
    "# 0 - ничья, 1 - победа домашней команды, 2 - победа гостевой команды\n",
    "data['goals_diff'] = data['ht_goals'] - data['at_goals']\n",
    "data['winner'] = data['goals_diff'].apply(lambda x: 0 if x == 0 else (1 if x > 0 else 2))\n",
    "data = data.drop(['goals_diff'], axis=1)\n",
    "\n",
    "data = data[correct_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для вычисления средних показателей по сезону (Турнирное положение)\n",
    "# Определяем team_id - домашняя или гостевая команда, в зависимости от этого выбираем нужные фичи\n",
    "# Еще вернем общее количество сыгранных матчей\n",
    "\n",
    "def get_result_before_match(team_id, match_list, cols_names):\n",
    "    ht_matches = match_list[match_list.ht_id == team_id]\n",
    "    at_matches = match_list[match_list.at_id == team_id]\n",
    "\n",
    "    ht_matches = ht_matches[columns_for_ht]\n",
    "    ht_matches.columns = cols_names\n",
    "\n",
    "    at_matches = at_matches[columns_for_at]\n",
    "    at_matches.columns = cols_names\n",
    "\n",
    "    mean_stats = pd.concat([ht_matches, at_matches]) \\\n",
    "        .add_prefix('mean_') \\\n",
    "        .mean() \\\n",
    "        .to_frame().T\n",
    "\n",
    "    mean_stats['matches_played'] = len(match_list)\n",
    "    \n",
    "    return mean_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления основных турнирных показателей: очки, количество забитых и пропущенных голов\n",
    "\n",
    "def get_stats(team_id, match_list, prefix, last_5=False):\n",
    "    points = 0\n",
    "    ht_matches = match_list[match_list.ht_id == team_id]\n",
    "    at_matches = match_list[match_list.at_id == team_id]    \n",
    "    \n",
    "    goals_scored = ht_matches.ht_goals.sum() + at_matches.at_goals.sum()\n",
    "    goals_conc = ht_matches.at_goals.sum() + at_matches.ht_goals.sum()\n",
    "    \n",
    "    away_goals = at_matches.at_goals - at_matches.ht_goals\n",
    "    home_goals = ht_matches.ht_goals - ht_matches.at_goals\n",
    "    \n",
    "    goals_diff = pd.concat([away_goals, home_goals]).tolist()\n",
    "    for diff in goals_diff:\n",
    "        if diff > 0:\n",
    "            points += 3\n",
    "        elif diff == 0:\n",
    "            points += 1 \n",
    "    \n",
    "    df = pd.DataFrame({'points': points, \n",
    "               'goals_scored': goals_scored, \n",
    "               'goals_conc': goals_conc}, index=[0])\n",
    "    \n",
    "    prefix = f'{prefix}l5_' if last_5 else prefix\n",
    "    \n",
    "    df = df.add_prefix(prefix)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрабатываем все данные, используя ранее написанные функции.\n",
    "Это non-pandas-way, но реализовать это без цикла по каждой строке у меня не получилось. Если можете помочь, пишите, буду благодарен.\n",
    "\n",
    "Необходимо для каждой встречи рассчитать следующие показатели:\n",
    "    1. Турнирное положение\n",
    "    2. Средние показатели по каждой фиче в зависимости от места проведения игры (домашняя/выездная)\n",
    "    3. Для определения формы команды - Турнирное положение за последние 5 матчей\n",
    "    \n",
    "В дальнейшей этот список можно будет расширить, используя все те же функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012/2013 done\n",
      "2013/2014 done\n",
      "2014/2015 done\n",
      "2015/2016 done\n",
      "2016/2017 done\n",
      "2017/2018 done\n",
      "2018/2019 done\n",
      "2019/2020 done\n"
     ]
    }
   ],
   "source": [
    "seasons = np.unique(data.season)\n",
    "stats = pd.DataFrame()\n",
    "for season in seasons:\n",
    "    curr_season = data[data.season == season] \\\n",
    "        .sort_values(by='match_date')\n",
    "    \n",
    "    match_info = curr_season.iloc[:, :9]\n",
    "    \n",
    "    l5_stats = pd.DataFrame()\n",
    "    mean_stats = pd.DataFrame()\n",
    "    standing = pd.DataFrame()\n",
    "    for i in range(len(curr_season)):\n",
    "        curr_match_id = curr_season.iloc[i, 1]\n",
    "        curr_date = curr_season.iloc[i, 2]\n",
    "        ht_id = curr_season.iloc[i, 4]\n",
    "        at_id = curr_season.iloc[i, 6]\n",
    "        \n",
    "        matches_b4_today = curr_season[curr_season.match_date < curr_date]        \n",
    "        ht_matches_b4_today = matches_b4_today[(matches_b4_today.ht_id == ht_id) | (matches_b4_today.at_id == ht_id)]\n",
    "        at_matches_b4_today = matches_b4_today[(matches_b4_today.ht_id == at_id) | (matches_b4_today.at_id == at_id)]\n",
    "\n",
    "        ht_mean = get_result_before_match(ht_id, ht_matches_b4_today, columns_for_ht)\n",
    "        at_mean = get_result_before_match(at_id, at_matches_b4_today, columns_for_at)\n",
    "        ht_mean.columns = ht_mean.columns.str.replace('matches_played', 'ht_matches_played')\n",
    "        at_mean.columns = at_mean.columns.str.replace('matches_played', 'at_matches_played')\n",
    "        \n",
    "        # l5 = 5 последних матчей\n",
    "        ht_l5 = get_stats(ht_id, ht_matches_b4_today.iloc[-5:, :], 'ht_', True)\n",
    "        at_l5 = get_stats(at_id, at_matches_b4_today.iloc[-5:, :], 'at_', True)\n",
    "        l5_df = pd.concat([ht_l5, at_l5], axis=1)\n",
    "        l5_df['match_id'] = curr_match_id\n",
    "        \n",
    "        # Турнирное положение на данный момент\n",
    "        ht_standing = get_stats(ht_id, ht_matches_b4_today, 'ht_')\n",
    "        at_standing = get_stats(at_id, at_matches_b4_today, 'at_')\n",
    "        standing_df = pd.concat([ht_standing, at_standing], axis=1)\n",
    "        standing_df['match_id'] = curr_match_id          \n",
    "\n",
    "        mean_df = pd.concat([ht_mean, at_mean], axis=1)\n",
    "        mean_df['match_id'] = curr_match_id\n",
    "        \n",
    "        l5_stats = l5_stats.append(l5_df)\n",
    "        mean_stats = mean_stats.append(mean_df)\n",
    "        standing = standing.append(standing_df)\n",
    "        \n",
    "    full_season_stats = match_info \\\n",
    "        .merge(mean_stats, on='match_id') \\\n",
    "        .merge(l5_stats, on='match_id') \\\n",
    "        .merge(standing, on='match_id')\n",
    "        \n",
    "    stats = stats.append(full_season_stats)\n",
    "    print(f'{season} done')\n",
    "    \n",
    "stats = stats.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>match_id</th>\n",
       "      <th>match_date</th>\n",
       "      <th>ht_name</th>\n",
       "      <th>ht_id</th>\n",
       "      <th>at_name</th>\n",
       "      <th>at_id</th>\n",
       "      <th>ht_goals</th>\n",
       "      <th>at_goals</th>\n",
       "      <th>mean_ht_goals</th>\n",
       "      <th>...</th>\n",
       "      <th>ht_l5_goals_conc</th>\n",
       "      <th>at_l5_points</th>\n",
       "      <th>at_l5_goals_scored</th>\n",
       "      <th>at_l5_goals_conc</th>\n",
       "      <th>ht_points</th>\n",
       "      <th>ht_goals_scored</th>\n",
       "      <th>ht_goals_conc</th>\n",
       "      <th>at_points</th>\n",
       "      <th>at_goals_scored</th>\n",
       "      <th>at_goals_conc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2012/2013</td>\n",
       "      <td>651496</td>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Ювентус</td>\n",
       "      <td>87</td>\n",
       "      <td>Парма</td>\n",
       "      <td>82</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2012/2013</td>\n",
       "      <td>651494</td>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Фиорентина</td>\n",
       "      <td>73</td>\n",
       "      <td>Удинезе</td>\n",
       "      <td>86</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2012/2013</td>\n",
       "      <td>651493</td>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Кьево</td>\n",
       "      <td>267</td>\n",
       "      <td>Болонья</td>\n",
       "      <td>71</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2012/2013</td>\n",
       "      <td>651495</td>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Дженоа</td>\n",
       "      <td>278</td>\n",
       "      <td>Кальяри</td>\n",
       "      <td>78</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2012/2013</td>\n",
       "      <td>651501</td>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Сиена</td>\n",
       "      <td>773</td>\n",
       "      <td>Торино</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      season  match_id match_date     ht_name  ht_id  at_name  at_id  \\\n",
       "0  2012/2013    651496 2012-08-25     Ювентус     87    Парма     82   \n",
       "1  2012/2013    651494 2012-08-25  Фиорентина     73  Удинезе     86   \n",
       "2  2012/2013    651493 2012-08-26       Кьево    267  Болонья     71   \n",
       "3  2012/2013    651495 2012-08-26      Дженоа    278  Кальяри     78   \n",
       "4  2012/2013    651501 2012-08-26       Сиена    773   Торино     72   \n",
       "\n",
       "   ht_goals  at_goals  mean_ht_goals  ...  ht_l5_goals_conc  at_l5_points  \\\n",
       "0       2.0       0.0            NaN  ...               0.0             0   \n",
       "1       2.0       1.0            NaN  ...               0.0             0   \n",
       "2       2.0       0.0            NaN  ...               0.0             0   \n",
       "3       2.0       0.0            NaN  ...               0.0             0   \n",
       "4       0.0       0.0            NaN  ...               0.0             0   \n",
       "\n",
       "   at_l5_goals_scored  at_l5_goals_conc  ht_points  ht_goals_scored  \\\n",
       "0                 0.0               0.0          0              0.0   \n",
       "1                 0.0               0.0          0              0.0   \n",
       "2                 0.0               0.0          0              0.0   \n",
       "3                 0.0               0.0          0              0.0   \n",
       "4                 0.0               0.0          0              0.0   \n",
       "\n",
       "   ht_goals_conc  at_points  at_goals_scored  at_goals_conc  \n",
       "0            0.0          0              0.0            0.0  \n",
       "1            0.0          0              0.0            0.0  \n",
       "2            0.0          0              0.0            0.0  \n",
       "3            0.0          0              0.0            0.0  \n",
       "4            0.0          0              0.0            0.0  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Некоторые фичи необходимо преобразовать, упущение при сборе данных)\n",
    "stats.mean_ht_drible_suc = stats.mean_ht_drible_suc / 100\n",
    "stats.mean_at_drible_suc = stats.mean_at_drible_suc / 100\n",
    "\n",
    "stats.mean_ht_otbor_perc = stats.mean_ht_otbor_perc / 100\n",
    "stats.mean_at_otbor_perc = stats.mean_at_otbor_perc / 100\n",
    "\n",
    "stats.mean_ht_possession = stats.mean_ht_possession / 100\n",
    "stats.mean_at_possession = stats.mean_at_possession / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим первые 5 матчей каждого сезона, т.к. они малоинформативны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = stats[(stats.ht_matches_played >= 5) | (stats.at_matches_played >= 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем решить задачу классификации победителя матча разными методами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['goals_diff'] = stats['ht_goals'] - stats['at_goals']\n",
    "stats['winner'] = stats['goals_diff'].apply(lambda x: 0 if x == 0 else (1 if x > 0 else 2))\n",
    "stats = stats.drop(['goals_diff'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2], dtype=int64), array([ 669, 1178,  796], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(stats.winner, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированны. Для баланса воспользуемся SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stats.loc[:, 'mean_ht_goals':'at_goals_conc']\n",
    "y = stats.winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_resampled, y_resampled = SMOTE().fit_resample(x, y)\n",
    "np.unique(y_resampled, return_counts=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_resampled, y_resampled = RandomUnderSampler().fit_resample(x, y)\n",
    "# np.unique(y_resampled, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test, y_pred):\n",
    "    pr, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    print(f'Prec = {round(pr, 3)} \\n Rec = {round(rec, 3)} \\n  F1 = {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec = 0.501 \n",
      " Rec = 0.495 \n",
      "  F1 = 0.494\n"
     ]
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=8)\n",
    "kn.fit(x_train, y_train)\n",
    "\n",
    "y_pred = kn.predict(x_test)\n",
    "get_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec = 0.567 \n",
      " Rec = 0.566 \n",
      "  F1 = 0.565\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(), SVC(gamma='auto', probability=True))\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(x_test)\n",
    "\n",
    "get_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec = 0.608 \n",
      " Rec = 0.607 \n",
      "  F1 = 0.607\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=0.05, n_estimators=300, min_samples_split=5,\n",
    "                                 min_samples_leaf=3, max_depth=5)\n",
    "gbc.fit(x_train, y_train)\n",
    "y_pred = gbc.predict(x_test)\n",
    "\n",
    "get_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры для методов подбирались с помощью RandomizedSearchCV. Наиболее продуктивная модель GradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного поменяем задачу. Сведем задачу к предсказанию количества голов команд. Ну а далее, кто больше забил - тот и победил. Данный способ напрямую не позволяет получить ничейный результат. Поэтому введем ограничения: если разница голов команд менее, чем 0.15 - признаем ничью.\n",
    "<br><br>\n",
    "Для этого выберем фичи, которые напрямую могут влиять на эффективность нападения команд.\n",
    "Так же рассчитаем среднюю эффективность ударов и сумму атак в целом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ht(data):\n",
    "    for_ht_regr = data[['mean_ht_wc_rating_avg', 'mean_ht_key_passes', 'mean_ht_corner', \n",
    "                        'ht_l5_points', 'ht_l5_goals_scored', 'mean_at_wc_rating_avg', \n",
    "                        'at_l5_points', 'at_l5_goals_conc']]\n",
    "    \n",
    "    for_ht_regr['attacks'] = data[['mean_ht_att_combination', 'mean_ht_stand_pol', 'mean_ht_contr_att', \n",
    "          'mean_ht_penalty', 'mean_ht_own_goal']].sum(axis=1)\n",
    "    \n",
    "    for_ht_regr['shots_eff'] = (data[['mean_ht_shot_in_target', 'mean_ht_shot_miss', \n",
    "                                  'mean_ht_shot_blocked']].sum(axis=1)) * data.mean_ht_goals\n",
    "    return for_ht_regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_at(data):\n",
    "    for_at_regr = data[['mean_at_wc_rating_avg', 'mean_at_key_passes', 'mean_at_corner', \n",
    "                        'at_l5_points', 'at_l5_goals_scored', 'mean_ht_wc_rating_avg', \n",
    "                        'ht_l5_points', 'ht_l5_goals_conc']]\n",
    "    \n",
    "    for_at_regr['attacks'] = data[['mean_at_att_combination', 'mean_at_stand_pol', 'mean_at_contr_att', \n",
    "          'mean_at_penalty', 'mean_at_own_goal']].sum(axis=1)\n",
    "    \n",
    "    for_at_regr['shots_eff'] = (data[['mean_at_shot_in_target', 'mean_at_shot_miss', \n",
    "                                  'mean_at_shot_blocked']].sum(axis=1)) * data.mean_at_goals\n",
    "    return for_at_regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regr_metrics(y_test, y_pred):\n",
    "    print(f'MSE  = {mean_squared_error(y_test, y_pred)}')\n",
    "    print(f'R2   = {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, необходимо 2 модели:\n",
    "    1. Для домашней команды\n",
    "    2. Для гостевой команды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = stats.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  = 1.2176347544173098\n",
      "R2   = 0.11307048885197102\n"
     ]
    }
   ],
   "source": [
    "x = get_data_ht(stats)\n",
    "y = stats.ht_goals\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=16)\n",
    "\n",
    "ht_clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1], cv=5).fit(x_train, y_train)\n",
    "y_pred_ht = ht_clf.predict(x_test)\n",
    "\n",
    "get_regr_metrics(y_test, y_pred_ht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично для гостевой команды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  = 1.2007155737978195\n",
      "R2   = 0.11499139541084424\n"
     ]
    }
   ],
   "source": [
    "x = get_data_at(stats)\n",
    "y = stats.at_goals\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=16)\n",
    "\n",
    "at_clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1], cv=5).fit(x_train, y_train)\n",
    "y_pred_at = at_clf.predict(x_test)\n",
    "\n",
    "get_regr_metrics(y_test, y_pred_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим победителя и сравним с результатами классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\n",
    "    'ht_goals_pred': y_pred_ht,\n",
    "    'at_goals_pred': y_pred_at\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "\n",
    "prediction['goals_diff'] = prediction['ht_goals_pred'] - prediction['at_goals_pred']\n",
    "prediction['winner'] = prediction['goals_diff'].apply(\n",
    "    lambda x: 0 if ((x > -threshold) & (x < threshold)) else (1 if x >= threshold else 2)\n",
    ")\n",
    "prediction = prediction.drop(['goals_diff'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = stats.loc[x_test.index, :].winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2], dtype=int64), array([135, 281, 113], dtype=int64))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(prediction.winner, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec = 0.505 \n",
      " Rec = 0.503 \n",
      "  F1 = 0.499\n"
     ]
    }
   ],
   "source": [
    "get_metrics(y_test, prediction.winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr = CatBoostRegressor(learning_rate=0.01, max_depth=7, iterations=500, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  = 1.229497734790739\n",
      "R2   = 0.10442945150871685\n"
     ]
    }
   ],
   "source": [
    "x = get_data_ht(stats)\n",
    "y = stats.ht_goals\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=16)\n",
    "\n",
    "ht_clf = cbr.fit(x_train, y_train)\n",
    "y_pred_ht = ht_clf.predict(x_test)\n",
    "\n",
    "get_regr_metrics(y_test, y_pred_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>mean_at_wc_rating_avg</td>\n",
       "      <td>19.067526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>shots_eff</td>\n",
       "      <td>15.262149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>mean_ht_wc_rating_avg</td>\n",
       "      <td>10.828919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>attacks</td>\n",
       "      <td>8.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>at_l5_points</td>\n",
       "      <td>8.351819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>at_l5_goals_conc</td>\n",
       "      <td>8.329513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ht_l5_goals_scored</td>\n",
       "      <td>8.308885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>mean_ht_key_passes</td>\n",
       "      <td>7.967409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ht_l5_points</td>\n",
       "      <td>6.916374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>mean_ht_corner</td>\n",
       "      <td>6.490506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature        imp\n",
       "5  mean_at_wc_rating_avg  19.067526\n",
       "9              shots_eff  15.262149\n",
       "0  mean_ht_wc_rating_avg  10.828919\n",
       "8                attacks   8.476900\n",
       "6           at_l5_points   8.351819\n",
       "7       at_l5_goals_conc   8.329513\n",
       "4     ht_l5_goals_scored   8.308885\n",
       "1     mean_ht_key_passes   7.967409\n",
       "3           ht_l5_points   6.916374\n",
       "2         mean_ht_corner   6.490506"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'feature' : cbr.feature_names_,\n",
    "    'imp' : cbr.feature_importances_\n",
    "}).sort_values(by='imp', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  = 1.2274551200982011\n",
      "R2   = 0.10591729935811833\n"
     ]
    }
   ],
   "source": [
    "x = get_data_at(stats)\n",
    "y = stats.ht_goals\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=16)\n",
    "\n",
    "ht_clf = cbr.fit(x_train, y_train)\n",
    "y_pred_ht = ht_clf.predict(x_test)\n",
    "\n",
    "get_regr_metrics(y_test, y_pred_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>mean_ht_wc_rating_avg</td>\n",
       "      <td>22.976563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>mean_at_wc_rating_avg</td>\n",
       "      <td>13.209719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>ht_l5_points</td>\n",
       "      <td>11.192796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>mean_at_corner</td>\n",
       "      <td>8.881780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>attacks</td>\n",
       "      <td>8.249086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>mean_at_key_passes</td>\n",
       "      <td>8.074265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>at_l5_points</td>\n",
       "      <td>8.016128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>shots_eff</td>\n",
       "      <td>7.215244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>at_l5_goals_scored</td>\n",
       "      <td>6.265318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ht_l5_goals_conc</td>\n",
       "      <td>5.919100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature        imp\n",
       "5  mean_ht_wc_rating_avg  22.976563\n",
       "0  mean_at_wc_rating_avg  13.209719\n",
       "6           ht_l5_points  11.192796\n",
       "2         mean_at_corner   8.881780\n",
       "8                attacks   8.249086\n",
       "1     mean_at_key_passes   8.074265\n",
       "3           at_l5_points   8.016128\n",
       "9              shots_eff   7.215244\n",
       "4     at_l5_goals_scored   6.265318\n",
       "7       ht_l5_goals_conc   5.919100"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'feature' : cbr.feature_names_,\n",
    "    'imp' : cbr.feature_importances_\n",
    "}).sort_values(by='imp', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном этапе итог работы таков:\n",
    "<ul>\n",
    "    <li>Точность метода с использованием регрессии ниже. Метрики для оценки качества регрессии здесь не помогают, ибо при предсказании, например 2:0 и действительном результате 5:0, ошибка будет большая, но по факту - победитель предсказан. Именно поэтому оценка производится приведением результатов к тому же виду, что и результаты классификации. </li><br>\n",
    "    <li>Если копаться не только в метриках, но еще и смотреть на сами данные, можно заметить, что большинство моделей способны предсказать наиболее очевидные результаты, когда играет заведомо разные по статистике команды, где явно видно преимущество одной над другой. На таких матчах в большинстве случаев коэффициент (в будущем просто кэф) довольно таки низкий. Из этого следует другой вывод. </li><br>\n",
    "    <li>Метрики для оценивания моделей необходимо привязывать к коэффициентам букмекеров. Мало смысла, когда модель предсказывает победителя с кэфом менее 1.2, а в матчах с кэфом более 2 ошибается. В таком случае лучше один верно предсказанный матч с кэфом 1.5, чем 4 матча с кэфом 1.1. Но и оставлять кэф как фичу вредно, т.к. именно эта фича становится самой важной при предсказании. В таком случае вся модель скатывается к правилу - победит тот, на кого кэф меньше).</li><br>\n",
    "    <li>Необходимо использовать данные для получения вероятностей вроде:\n",
    "<p>При преимуществе домашней команды в среднем количестве атак и среднему количеству голов (где не всегда наблюдается корреляция :)) над гостевой командой, гостевая команды выиграла только 14% матчей, при этом в половине из них она не забила.</p>\n",
    "И отталкиваясь от таких статистических закономерностей строить свою стратегию.</li><br>\n",
    "    <li>Несмотря на метрики, на данных по текущему сезону, лучше всего отрабатывает подход с использованием регрессии. Его точность по-прежнему чуть выше 0.5, но часто проходят прогнозы с кэфом более 3. Т.е. пока не наблюдается предсказание \"очевидных матчей\". С точки зрения метрик модель хуже, но с финансовой точки зрения - лучше.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
